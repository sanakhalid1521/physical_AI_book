---
title: "Lesson 3.3: Learning-Based Control"
sidebar_label: "Learning-Based Control"
description: "Introduction to reinforcement learning, imitation learning, and adaptive control methods for humanoid robots"
keywords: ["reinforcement learning", "imitation learning", "adaptive control", "robot learning"]
---

# Lesson 3.3: Learning-Based Control

## Introduction

Learning-based control methods enable humanoid robots to improve their performance through experience and interaction with the environment. This lesson explores reinforcement learning, imitation learning, and adaptive control techniques that allow robots to learn complex behaviors, adapt to new situations, and improve their control policies over time.

## Learning Objectives

After completing this lesson, students will be able to:
- Understand the fundamentals of reinforcement learning for robot control
- Implement basic imitation learning algorithms
- Apply adaptive control techniques for humanoid robots

## Prerequisites

Students should have knowledge of:
- Basic understanding of machine learning concepts
- Programming skills in Python
- Fundamentals of control theory and robotics

## Theoretical Foundation

Learning-based control methods allow robots to acquire behaviors through interaction with their environment rather than relying solely on pre-programmed responses. These approaches are particularly valuable for humanoid robots that need to operate in complex, unstructured environments.

### Key Concepts

- **Reinforcement Learning (RL)**: Learning through trial and error with reward signals
- **Imitation Learning**: Learning by observing and mimicking expert demonstrations
- **Adaptive Control**: Adjusting control parameters based on system performance

## Practical Application

Let's explore learning-based control methods with practical examples:

### Example 1: Q-Learning for Simple Robot Control

```python
import numpy as np
import random
from typing import Tuple, List
import matplotlib.pyplot as plt

class QLearningAgent:
    def __init__(self, state_space_size: Tuple[int, int], action_space_size: int,
                 learning_rate: float = 0.1, discount_factor: float = 0.95,
                 exploration_rate: float = 0.1):
        self.state_space_size = state_space_size
        self.action_space_size = action_space_size
        self.learning_rate = learning_rate
        self.discount_factor = discount_factor
        self.exploration_rate = exploration_rate

        # Initialize Q-table
        self.q_table = np.zeros((*state_space_size, action_space_size))

    def get_action(self, state: Tuple[int, int]) -> int:
        """Choose action using epsilon-greedy policy"""
        if random.random() < self.exploration_rate:
            # Explore: random action
            return random.randint(0, self.action_space_size - 1)
        else:
            # Exploit: best known action
            return int(np.argmax(self.q_table[state]))

    def update_q_value(self, state: Tuple[int, int], action: int,
                       reward: float, next_state: Tuple[int, int]):
        """Update Q-value using Bellman equation"""
        current_q = self.q_table[state][action]

        # Calculate target Q-value
        max_next_q = np.max(self.q_table[next_state])
        target_q = reward + self.discount_factor * max_next_q

        # Update Q-value
        self.q_table[state][action] += self.learning_rate * (target_q - current_q)

class GridWorldEnvironment:
    def __init__(self, width: int, height: int):
        self.width = width
        self.height = height
        self.agent_pos = [0, 0]  # Start at top-left
        self.goal_pos = [width-1, height-1]  # Goal at bottom-right
        self.obstacles = [[2, 1], [2, 2], [3, 2]]  # Fixed obstacles

        # Action space: 0=up, 1=right, 2=down, 3=left
        self.actions = [(-1, 0), (0, 1), (1, 0), (0, -1)]

    def reset(self):
        """Reset environment to initial state"""
        self.agent_pos = [0, 0]
        return tuple(self.agent_pos)

    def step(self, action: int) -> Tuple[Tuple[int, int], float, bool]:
        """Execute action and return (next_state, reward, done)"""
        # Calculate new position
        new_pos = [
            self.agent_pos[0] + self.actions[action][0],
            self.agent_pos[1] + self.actions[action][1]
        ]

        # Check boundaries
        if (0 <= new_pos[0] < self.width and 0 <= new_pos[1] < self.height):
            # Check for obstacles
            if new_pos not in self.obstacles:
                self.agent_pos = new_pos

        # Calculate reward
        if self.agent_pos == self.goal_pos:
            reward = 100  # Reached goal
            done = True
        elif self.agent_pos in self.obstacles:
            reward = -10  # Hit obstacle
            done = False
        else:
            reward = -1  # Time penalty
            done = False

        return tuple(self.agent_pos), reward, done

# Example usage
env = GridWorldEnvironment(5, 5)
agent = QLearningAgent(state_space_size=(5, 5), action_space_size=4)

# Training loop
episodes = 1000
episode_rewards = []

for episode in range(episodes):
    state = env.reset()
    total_reward = 0
    done = False

    while not done:
        action = agent.get_action(state)
        next_state, reward, done = env.step(action)
        agent.update_q_value(state, action, reward, next_state)
        state = next_state
        total_reward += reward

    episode_rewards.append(total_reward)

print(f"Training completed. Final average reward: {np.mean(episode_rewards[-100:]):.2f}")
```

### Example 2: Imitation Learning for Robot Behavior

```python
import numpy as np
from typing import List, Tuple
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler

class ImitationLearningAgent:
    def __init__(self):
        self.demonstrations = []  # List of (state, action) pairs
        self.model = LinearRegression()
        self.scaler = StandardScaler()
        self.trained = False

    def add_demonstration(self, states: List[np.ndarray], actions: List[np.ndarray]):
        """Add a demonstration trajectory"""
        for state, action in zip(states, actions):
            self.demonstrations.append((state, action))

    def train(self):
        """Train the imitation learning model"""
        if len(self.demonstrations) == 0:
            print("No demonstrations available for training")
            return

        # Prepare training data
        X = np.array([demo[0] for demo in self.demonstrations])
        y = np.array([demo[1] for demo in self.demonstrations])

        # Normalize features
        X_normalized = self.scaler.fit_transform(X)

        # Train the model
        self.model.fit(X_normalized, y)
        self.trained = True
        print(f"Trained on {len(self.demonstrations)} demonstration samples")

    def predict_action(self, state: np.ndarray) -> np.ndarray:
        """Predict action for given state"""
        if not self.trained:
            print("Model not trained yet")
            return np.zeros_like(state)  # Return zero action if not trained

        # Normalize state
        state_normalized = self.scaler.transform([state])

        # Predict action
        predicted_action = self.model.predict(state_normalized)[0]
        return predicted_action

class SimpleManipulationTask:
    """Simulated manipulation task for demonstration"""
    def __init__(self):
        self.object_pos = np.array([0.5, 0.3, 0.1])  # Object position
        self.robot_pos = np.array([0.0, 0.0, 0.5])   # Robot end-effector position
        self.goal_pos = np.array([0.8, 0.7, 0.2])    # Goal position

    def get_state(self) -> np.ndarray:
        """Get current state (relative positions)"""
        obj_to_robot = self.object_pos - self.robot_pos
        robot_to_goal = self.goal_pos - self.robot_pos
        return np.concatenate([obj_to_robot, robot_to_goal])

    def execute_action(self, action: np.ndarray):
        """Execute action and update environment"""
        # Simple movement: update robot position
        self.robot_pos += action * 0.05  # Scale down action
        # Keep within bounds
        self.robot_pos = np.clip(self.robot_pos, 0, 1)

    def is_task_complete(self) -> bool:
        """Check if task is complete"""
        return np.linalg.norm(self.robot_pos - self.goal_pos) < 0.1

def generate_demonstration():
    """Generate a demonstration trajectory"""
    task = SimpleManipulationTask()
    states = []
    actions = []

    # Simple demonstration: move toward object, then to goal
    for _ in range(20):  # Move toward object
        state = task.get_state()
        # Simple policy: move toward object
        action = task.object_pos - task.robot_pos
        action = action / (np.linalg.norm(action) + 1e-6)  # Normalize
        states.append(state)
        actions.append(action)
        task.execute_action(action)

    # Reset task for second part
    task.robot_pos = task.object_pos  # Start from object position

    for _ in range(20):  # Move to goal
        state = task.get_state()
        # Simple policy: move toward goal
        action = task.goal_pos - task.robot_pos
        action = action / (np.linalg.norm(action) + 1e-6)  # Normalize
        states.append(state)
        actions.append(action)
        task.execute_action(action)

    return states, actions

# Example usage
imitation_agent = ImitationLearningAgent()

# Generate and add demonstrations
print("Generating demonstrations...")
for i in range(5):  # Create 5 demonstration trajectories
    states, actions = generate_demonstration()
    imitation_agent.add_demonstration(states, actions)

# Train the agent
print("Training imitation learning agent...")
imitation_agent.train()

# Test the trained agent
print("Testing trained agent...")
test_task = SimpleManipulationTask()
for step in range(30):
    state = test_task.get_state()
    action = imitation_agent.predict_action(state)
    test_task.execute_action(action)

    if test_task.is_task_complete():
        print(f"Task completed at step {step}")
        break

print(f"Final distance to goal: {np.linalg.norm(test_task.robot_pos - test_task.goal_pos):.3f}")
```

## Implementation Guide

### Step 1: Setup

Install necessary libraries for learning-based control:

```bash
pip install numpy scikit-learn matplotlib
```

### Step 2: Implementation

1. Define state and action spaces for the learning problem
2. Implement the learning algorithm (RL, imitation, or adaptive)
3. Create reward functions or demonstration collection methods
4. Add policy evaluation and improvement mechanisms

### Step 3: Testing

Validate your implementation by:
- Testing learning convergence with simple problems
- Verifying policy performance in simulation
- Measuring learning efficiency and sample complexity

## Hands-on Exercise

Implement a learning-based control system:

1. **Task 1**: Create a Q-learning agent for a simple navigation task
2. **Task 2**: Implement an imitation learning system for basic behaviors
3. **Task 3**: Add adaptive control mechanisms for parameter tuning

### Exercise Requirements

- Support both exploration and exploitation in learning
- Include proper reward shaping for effective learning
- Implement performance evaluation metrics

## Verification

How to test and validate the implementation:

- Test learning convergence with known problems
- Verify that learned policies outperform random policies
- Measure sample efficiency and learning speed

## Troubleshooting Guide

Common issues and solutions:

- **Issue 1**: Learning not converging or taking too long
  - Solution: Adjust learning rate, exploration rate, or reward structure
- **Issue 2**: Poor generalization to new situations
  - Solution: Collect more diverse demonstrations or use function approximation
- **Issue 3**: Instability in adaptive control
  - Solution: Add stability constraints and proper parameter bounds

## Real-World Relevance

Learning-based control is essential in humanoid robotics for:
- Adapting to new environments and tasks
- Improving performance through experience
- Handling uncertain and dynamic conditions
- Personalizing robot behavior to users

## Safety Considerations

When working with learning-based control systems:
- Implement safety constraints to prevent dangerous behaviors
- Use safe exploration techniques during learning
- Validate learned policies before deployment

## Further Exploration

Advanced topics and additional resources for students who want to dive deeper:

- Study of deep reinforcement learning for complex control
- Research into inverse reinforcement learning
- Exploration of meta-learning for rapid adaptation
- Investigation of human-in-the-loop learning approaches

## Summary

This lesson covered the fundamentals of learning-based control for humanoid robots, including reinforcement learning, imitation learning, and adaptive control methods that enable robots to learn and improve their behaviors over time.

## Knowledge Check

- Question 1: What are the main differences between reinforcement learning and imitation learning?
- Question 2: How does adaptive control differ from traditional control methods?
- Question 3: What are the challenges of applying learning-based methods to physical robots?