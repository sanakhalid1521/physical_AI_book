"use strict";(globalThis.webpackChunklearningbook=globalThis.webpackChunklearningbook||[]).push([[934],{8404:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>p,frontMatter:()=>s,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"chapters/perception/computer-vision-for-robotics","title":"Lesson 2.2: Computer Vision for Robotics","description":"Object detection, recognition, and tracking in robotic contexts, depth estimation, and visual servoing for humanoid robots","source":"@site/docs/chapters/02-perception/02-computer-vision-for-robotics.mdx","sourceDirName":"chapters/02-perception","slug":"/chapters/perception/computer-vision-for-robotics","permalink":"/docs/chapters/perception/computer-vision-for-robotics","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/chapters/02-perception/02-computer-vision-for-robotics.mdx","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"title":"Lesson 2.2: Computer Vision for Robotics","sidebar_label":"Computer Vision for Robotics","description":"Object detection, recognition, and tracking in robotic contexts, depth estimation, and visual servoing for humanoid robots","keywords":["computer vision","robotics vision","object detection","visual servoing"]},"sidebar":"docsSidebar","previous":{"title":"Sensor Integration and Data Processing","permalink":"/docs/chapters/perception/sensor-integration"},"next":{"title":"Multi-modal Perception","permalink":"/docs/chapters/perception/multi-modal-perception"}}');var t=i(4848),r=i(8453);const s={title:"Lesson 2.2: Computer Vision for Robotics",sidebar_label:"Computer Vision for Robotics",description:"Object detection, recognition, and tracking in robotic contexts, depth estimation, and visual servoing for humanoid robots",keywords:["computer vision","robotics vision","object detection","visual servoing"]},a="Lesson 2.2: Computer Vision for Robotics",l={},c=[{value:"Introduction",id:"introduction",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Theoretical Foundation",id:"theoretical-foundation",level:2},{value:"Key Concepts",id:"key-concepts",level:3},{value:"Practical Application",id:"practical-application",level:2},{value:"Example 1: Basic Object Detection for Robotics",id:"example-1-basic-object-detection-for-robotics",level:3},{value:"Example 2: Visual Servoing for Robot Control",id:"example-2-visual-servoing-for-robot-control",level:3},{value:"Implementation Guide",id:"implementation-guide",level:2},{value:"Step 1: Setup",id:"step-1-setup",level:3},{value:"Step 2: Implementation",id:"step-2-implementation",level:3},{value:"Step 3: Testing",id:"step-3-testing",level:3},{value:"Hands-on Exercise",id:"hands-on-exercise",level:2},{value:"Exercise Requirements",id:"exercise-requirements",level:3},{value:"Verification",id:"verification",level:2},{value:"Troubleshooting Guide",id:"troubleshooting-guide",level:2},{value:"Real-World Relevance",id:"real-world-relevance",level:2},{value:"Safety Considerations",id:"safety-considerations",level:2},{value:"Further Exploration",id:"further-exploration",level:2},{value:"Summary",id:"summary",level:2},{value:"Knowledge Check",id:"knowledge-check",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"lesson-22-computer-vision-for-robotics",children:"Lesson 2.2: Computer Vision for Robotics"})}),"\n",(0,t.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,t.jsx)(n.p,{children:"Computer vision is a critical component of humanoid robotics, enabling robots to perceive and interpret visual information from their environment. This lesson explores the application of computer vision techniques specifically tailored for robotic systems, including object detection, recognition, tracking, and visual servoing for precise manipulation tasks."}),"\n",(0,t.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsx)(n.p,{children:"After completing this lesson, students will be able to:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Understand the fundamentals of computer vision in robotic applications"}),"\n",(0,t.jsx)(n.li,{children:"Implement basic object detection and recognition algorithms"}),"\n",(0,t.jsx)(n.li,{children:"Apply visual servoing techniques for robot control"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,t.jsx)(n.p,{children:"Students should have knowledge of:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Basic understanding of image processing concepts"}),"\n",(0,t.jsx)(n.li,{children:"Programming skills in Python and OpenCV"}),"\n",(0,t.jsx)(n.li,{children:"Familiarity with linear algebra concepts"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"theoretical-foundation",children:"Theoretical Foundation"}),"\n",(0,t.jsx)(n.p,{children:"Computer vision in robotics differs from traditional computer vision in that it must operate in real-time, handle dynamic environments, and integrate seamlessly with robot control systems. The visual information must be processed to support navigation, manipulation, and interaction tasks."}),"\n",(0,t.jsx)(n.h3,{id:"key-concepts",children:"Key Concepts"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Visual Servoing"}),": Using visual feedback to control robot motion"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Real-time Processing"}),": Processing images at frame rates suitable for robot control"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"3D Reconstruction"}),": Estimating depth and 3D structure from 2D images"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"practical-application",children:"Practical Application"}),"\n",(0,t.jsx)(n.p,{children:"Let's explore computer vision techniques specifically for robotics applications:"}),"\n",(0,t.jsx)(n.h3,{id:"example-1-basic-object-detection-for-robotics",children:"Example 1: Basic Object Detection for Robotics"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import cv2\nimport numpy as np\nimport time\n\nclass RobotVisionSystem:\n    def __init__(self):\n        self.object_templates = {}\n        self.camera_matrix = None  # Camera intrinsic parameters\n        self.dist_coeffs = None    # Distortion coefficients\n\n    def detect_objects(self, image):\n        """Detect objects in the image and return their positions"""\n        # Convert to grayscale\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n        # Apply Gaussian blur to reduce noise\n        blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n\n        # Use Canny edge detection\n        edges = cv2.Canny(blurred, 50, 150)\n\n        # Find contours\n        contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n        objects = []\n        for contour in contours:\n            # Filter by area to avoid tiny contours\n            if cv2.contourArea(contour) > 100:\n                # Get bounding box\n                x, y, w, h = cv2.boundingRect(contour)\n\n                # Calculate center\n                center_x = x + w // 2\n                center_y = y + h // 2\n\n                # Approximate contour to polygon\n                epsilon = 0.02 * cv2.arcLength(contour, True)\n                approx = cv2.approxPolyDP(contour, epsilon, True)\n\n                # Determine object type based on shape\n                if len(approx) == 3:\n                    obj_type = "triangle"\n                elif len(approx) == 4:\n                    # Check if it\'s square or rectangle\n                    aspect_ratio = float(w) / h\n                    if 0.9 <= aspect_ratio <= 1.1:\n                        obj_type = "square"\n                    else:\n                        obj_type = "rectangle"\n                else:\n                    obj_type = "circle"  # Approximate other shapes as circles\n\n                objects.append({\n                    \'type\': obj_type,\n                    \'center\': (center_x, center_y),\n                    \'bbox\': (x, y, w, h),\n                    \'contour\': contour\n                })\n\n        return objects\n\n    def estimate_depth(self, left_image, right_image):\n        """Estimate depth using stereo vision"""\n        # Convert to grayscale\n        gray_left = cv2.cvtColor(left_image, cv2.COLOR_BGR2GRAY)\n        gray_right = cv2.cvtColor(right_image, cv2.COLOR_BGR2GRAY)\n\n        # Create stereo block matcher\n        stereo = cv2.StereoBM_create(numDisparities=16, blockSize=15)\n\n        # Compute disparity map\n        disparity = stereo.compute(gray_left, gray_right).astype(np.float32) / 16.0\n\n        # Convert disparity to depth (simplified)\n        # In practice, you\'d use camera calibration parameters\n        baseline = 0.1  # Camera baseline in meters\n        focal_length = 1000  # Focal length in pixels (example)\n        depth_map = (baseline * focal_length) / (disparity + 1e-6)  # Add small value to avoid division by zero\n\n        return depth_map\n\n# Example usage\nvision_system = RobotVisionSystem()\n\n# Simulate capturing an image (in practice, this would come from a camera)\n# For demonstration, we\'ll create a synthetic image\nsynthetic_image = np.zeros((480, 640, 3), dtype=np.uint8)\ncv2.rectangle(synthetic_image, (200, 150), (300, 250), (0, 255, 0), 2)  # Green rectangle\ncv2.circle(synthetic_image, (400, 200), 50, (255, 0, 0), 2)  # Blue circle\n\n# Detect objects\ndetected_objects = vision_system.detect_objects(synthetic_image)\n\nprint(f"Detected {len(detected_objects)} objects:")\nfor obj in detected_objects:\n    print(f"  Type: {obj[\'type\']}, Center: {obj[\'center\']}, BBox: {obj[\'bbox\']}")\n\n# Draw bounding boxes on image\nfor obj in detected_objects:\n    x, y, w, h = obj[\'bbox\']\n    cv2.rectangle(synthetic_image, (x, y), (x+w, y+h), (0, 255, 255), 2)\n    cv2.putText(synthetic_image, obj[\'type\'], (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 1)\n'})}),"\n",(0,t.jsx)(n.h3,{id:"example-2-visual-servoing-for-robot-control",children:"Example 2: Visual Servoing for Robot Control"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import numpy as np\nimport cv2\n\nclass VisualServoingController:\n    def __init__(self, target_x=320, target_y=240, tolerance=10):\n        self.target_x = target_x  # Target x-coordinate (center of image)\n        self.target_y = target_y  # Target y-coordinate (center of image)\n        self.tolerance = tolerance  # Tolerance for reaching target\n        self.gain = 0.01  # Control gain\n\n    def calculate_control_commands(self, object_position):\n        \"\"\"Calculate robot control commands based on object position\"\"\"\n        obj_x, obj_y = object_position\n\n        # Calculate errors\n        error_x = self.target_x - obj_x\n        error_y = self.target_y - obj_y\n\n        # Check if object is within tolerance\n        if abs(error_x) < self.tolerance and abs(error_y) < self.tolerance:\n            return {\n                'command': 'REACHED_TARGET',\n                'linear_x': 0,\n                'linear_y': 0,\n                'angular_z': 0\n            }\n\n        # Calculate control commands (simplified)\n        linear_x = self.gain * error_y  # Move forward/backward based on y-error\n        linear_y = self.gain * error_x  # Move left/right based on x-error\n        angular_z = self.gain * (-error_x)  # Rotate based on x-error\n\n        # Limit maximum velocities\n        max_vel = 0.5\n        linear_x = max(min(linear_x, max_vel), -max_vel)\n        linear_y = max(min(linear_y, max_vel), -max_vel)\n        angular_z = max(min(angular_z, max_vel), -max_vel)\n\n        return {\n            'command': 'MOVE_TO_OBJECT',\n            'linear_x': linear_x,\n            'linear_y': linear_y,\n            'angular_z': angular_z,\n            'error_x': error_x,\n            'error_y': error_y\n        }\n\n    def track_and_control(self, image, object_position):\n        \"\"\"Track object and generate control commands\"\"\"\n        # Calculate control commands\n        commands = self.calculate_control_commands(object_position)\n\n        # Draw target crosshair on image\n        cv2.line(image, (self.target_x - 20, self.target_y), (self.target_x + 20, self.target_y), (0, 255, 0), 2)\n        cv2.line(image, (self.target_x, self.target_y - 20), (self.target_x, self.target_y + 20), (0, 255, 0), 2)\n\n        # Draw object position\n        cv2.circle(image, object_position, 10, (255, 0, 0), 2)\n\n        # Draw line from object to target\n        cv2.line(image, object_position, (self.target_x, self.target_y), (255, 255, 0), 1)\n\n        return commands, image\n\n# Example usage\nservo_controller = VisualServoingController()\n\n# Simulate an image with an object at a specific position\nsimulated_image = np.zeros((480, 640, 3), dtype=np.uint8)\nobject_pos = (280, 200)  # Object position in image\n\n# Apply visual servoing\ncommands, annotated_image = servo_controller.track_and_control(simulated_image, object_pos)\n\nprint(f\"Control commands: {commands}\")\n"})}),"\n",(0,t.jsx)(n.h2,{id:"implementation-guide",children:"Implementation Guide"}),"\n",(0,t.jsx)(n.h3,{id:"step-1-setup",children:"Step 1: Setup"}),"\n",(0,t.jsx)(n.p,{children:"Install necessary computer vision libraries:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"pip install opencv-python numpy\n"})}),"\n",(0,t.jsx)(n.h3,{id:"step-2-implementation",children:"Step 2: Implementation"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Set up camera interface and calibration"}),"\n",(0,t.jsx)(n.li,{children:"Implement object detection algorithms"}),"\n",(0,t.jsx)(n.li,{children:"Create visual servoing control loop"}),"\n",(0,t.jsx)(n.li,{children:"Add 3D reconstruction capabilities"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"step-3-testing",children:"Step 3: Testing"}),"\n",(0,t.jsx)(n.p,{children:"Validate your implementation by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Testing object detection with various lighting conditions"}),"\n",(0,t.jsx)(n.li,{children:"Verifying visual servoing accuracy"}),"\n",(0,t.jsx)(n.li,{children:"Measuring processing frame rates"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"hands-on-exercise",children:"Hands-on Exercise"}),"\n",(0,t.jsx)(n.p,{children:"Implement a basic computer vision system for robotics:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Task 1"}),": Create an object detection system for simple geometric shapes"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Task 2"}),": Implement a visual servoing controller to track objects"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Task 3"}),": Add depth estimation using stereo vision (simulated)"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"exercise-requirements",children:"Exercise Requirements"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Support detection of at least 3 different object types"}),"\n",(0,t.jsx)(n.li,{children:"Implement real-time processing (30+ FPS)"}),"\n",(0,t.jsx)(n.li,{children:"Include basic visual servoing for object tracking"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"verification",children:"Verification"}),"\n",(0,t.jsx)(n.p,{children:"How to test and validate the implementation:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Test with various objects and lighting conditions"}),"\n",(0,t.jsx)(n.li,{children:"Verify that visual servoing achieves accurate positioning"}),"\n",(0,t.jsx)(n.li,{children:"Measure processing latency and throughput"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"troubleshooting-guide",children:"Troubleshooting Guide"}),"\n",(0,t.jsx)(n.p,{children:"Common issues and solutions:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Issue 1"}),": Object detection failing in varying lighting conditions","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Solution: Implement adaptive thresholding and preprocessing"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Issue 2"}),": Visual servoing oscillating around target","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Solution: Adjust control gains and add damping"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Issue 3"}),": High computational load affecting real-time performance","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Solution: Optimize algorithms and consider hardware acceleration"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"real-world-relevance",children:"Real-World Relevance"}),"\n",(0,t.jsx)(n.p,{children:"Computer vision is essential in humanoid robotics for:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Navigation and obstacle avoidance"}),"\n",(0,t.jsx)(n.li,{children:"Object recognition and manipulation"}),"\n",(0,t.jsx)(n.li,{children:"Human-robot interaction"}),"\n",(0,t.jsx)(n.li,{children:"Environmental mapping and localization"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"safety-considerations",children:"Safety Considerations"}),"\n",(0,t.jsx)(n.p,{children:"When working with computer vision systems:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Ensure proper camera calibration for accurate measurements"}),"\n",(0,t.jsx)(n.li,{children:"Implement fallback behaviors when vision fails"}),"\n",(0,t.jsx)(n.li,{children:"Consider the impact of lighting and environmental conditions"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"further-exploration",children:"Further Exploration"}),"\n",(0,t.jsx)(n.p,{children:"Advanced topics and additional resources for students who want to dive deeper:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Study of deep learning approaches for object detection (YOLO, SSD)"}),"\n",(0,t.jsx)(n.li,{children:"Research into 3D object pose estimation"}),"\n",(0,t.jsx)(n.li,{children:"Exploration of SLAM algorithms for mapping"}),"\n",(0,t.jsx)(n.li,{children:"Investigation of event-based vision systems"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,t.jsx)(n.p,{children:"This lesson covered the fundamentals of computer vision in robotics, including object detection, recognition, and visual servoing techniques specifically tailored for humanoid robot applications."}),"\n",(0,t.jsx)(n.h2,{id:"knowledge-check",children:"Knowledge Check"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Question 1: How does computer vision in robotics differ from traditional computer vision?"}),"\n",(0,t.jsx)(n.li,{children:"Question 2: What is visual servoing and how is it used in robotics?"}),"\n",(0,t.jsx)(n.li,{children:"Question 3: What are the challenges of real-time computer vision for robots?"}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>s,x:()=>a});var o=i(6540);const t={},r=o.createContext(t);function s(e){const n=o.useContext(r);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:s(e.components),o.createElement(r.Provider,{value:n},e.children)}}}]);