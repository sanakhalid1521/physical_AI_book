"use strict";(globalThis.webpackChunklearningbook=globalThis.webpackChunklearningbook||[]).push([[234],{1252:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>l,contentTitle:()=>s,default:()=>p,frontMatter:()=>r,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"chapters/control/learning-based-control","title":"Lesson 3.3: Learning-Based Control","description":"Introduction to reinforcement learning, imitation learning, and adaptive control methods for humanoid robots","source":"@site/docs/chapters/03-control/03-learning-based-control.mdx","sourceDirName":"chapters/03-control","slug":"/chapters/control/learning-based-control","permalink":"/docs/chapters/control/learning-based-control","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/chapters/03-control/03-learning-based-control.mdx","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"title":"Lesson 3.3: Learning-Based Control","sidebar_label":"Learning-Based Control","description":"Introduction to reinforcement learning, imitation learning, and adaptive control methods for humanoid robots","keywords":["reinforcement learning","imitation learning","adaptive control","robot learning"]},"sidebar":"docsSidebar","previous":{"title":"Behavioral Control Systems","permalink":"/docs/chapters/control/behavioral-control"},"next":{"title":"Planning and Reasoning","permalink":"/docs/chapters/intelligence/planning-and-reasoning"}}');var o=t(4848),a=t(8453);const r={title:"Lesson 3.3: Learning-Based Control",sidebar_label:"Learning-Based Control",description:"Introduction to reinforcement learning, imitation learning, and adaptive control methods for humanoid robots",keywords:["reinforcement learning","imitation learning","adaptive control","robot learning"]},s="Lesson 3.3: Learning-Based Control",l={},d=[{value:"Introduction",id:"introduction",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Theoretical Foundation",id:"theoretical-foundation",level:2},{value:"Key Concepts",id:"key-concepts",level:3},{value:"Practical Application",id:"practical-application",level:2},{value:"Example 1: Q-Learning for Simple Robot Control",id:"example-1-q-learning-for-simple-robot-control",level:3},{value:"Example 2: Imitation Learning for Robot Behavior",id:"example-2-imitation-learning-for-robot-behavior",level:3},{value:"Implementation Guide",id:"implementation-guide",level:2},{value:"Step 1: Setup",id:"step-1-setup",level:3},{value:"Step 2: Implementation",id:"step-2-implementation",level:3},{value:"Step 3: Testing",id:"step-3-testing",level:3},{value:"Hands-on Exercise",id:"hands-on-exercise",level:2},{value:"Exercise Requirements",id:"exercise-requirements",level:3},{value:"Verification",id:"verification",level:2},{value:"Troubleshooting Guide",id:"troubleshooting-guide",level:2},{value:"Real-World Relevance",id:"real-world-relevance",level:2},{value:"Safety Considerations",id:"safety-considerations",level:2},{value:"Further Exploration",id:"further-exploration",level:2},{value:"Summary",id:"summary",level:2},{value:"Knowledge Check",id:"knowledge-check",level:2}];function c(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...n.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(e.header,{children:(0,o.jsx)(e.h1,{id:"lesson-33-learning-based-control",children:"Lesson 3.3: Learning-Based Control"})}),"\n",(0,o.jsx)(e.h2,{id:"introduction",children:"Introduction"}),"\n",(0,o.jsx)(e.p,{children:"Learning-based control methods enable humanoid robots to improve their performance through experience and interaction with the environment. This lesson explores reinforcement learning, imitation learning, and adaptive control techniques that allow robots to learn complex behaviors, adapt to new situations, and improve their control policies over time."}),"\n",(0,o.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,o.jsx)(e.p,{children:"After completing this lesson, students will be able to:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Understand the fundamentals of reinforcement learning for robot control"}),"\n",(0,o.jsx)(e.li,{children:"Implement basic imitation learning algorithms"}),"\n",(0,o.jsx)(e.li,{children:"Apply adaptive control techniques for humanoid robots"}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,o.jsx)(e.p,{children:"Students should have knowledge of:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Basic understanding of machine learning concepts"}),"\n",(0,o.jsx)(e.li,{children:"Programming skills in Python"}),"\n",(0,o.jsx)(e.li,{children:"Fundamentals of control theory and robotics"}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"theoretical-foundation",children:"Theoretical Foundation"}),"\n",(0,o.jsx)(e.p,{children:"Learning-based control methods allow robots to acquire behaviors through interaction with their environment rather than relying solely on pre-programmed responses. These approaches are particularly valuable for humanoid robots that need to operate in complex, unstructured environments."}),"\n",(0,o.jsx)(e.h3,{id:"key-concepts",children:"Key Concepts"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Reinforcement Learning (RL)"}),": Learning through trial and error with reward signals"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Imitation Learning"}),": Learning by observing and mimicking expert demonstrations"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Adaptive Control"}),": Adjusting control parameters based on system performance"]}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"practical-application",children:"Practical Application"}),"\n",(0,o.jsx)(e.p,{children:"Let's explore learning-based control methods with practical examples:"}),"\n",(0,o.jsx)(e.h3,{id:"example-1-q-learning-for-simple-robot-control",children:"Example 1: Q-Learning for Simple Robot Control"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'import numpy as np\nimport random\nfrom typing import Tuple, List\nimport matplotlib.pyplot as plt\n\nclass QLearningAgent:\n    def __init__(self, state_space_size: Tuple[int, int], action_space_size: int,\n                 learning_rate: float = 0.1, discount_factor: float = 0.95,\n                 exploration_rate: float = 0.1):\n        self.state_space_size = state_space_size\n        self.action_space_size = action_space_size\n        self.learning_rate = learning_rate\n        self.discount_factor = discount_factor\n        self.exploration_rate = exploration_rate\n\n        # Initialize Q-table\n        self.q_table = np.zeros((*state_space_size, action_space_size))\n\n    def get_action(self, state: Tuple[int, int]) -> int:\n        """Choose action using epsilon-greedy policy"""\n        if random.random() < self.exploration_rate:\n            # Explore: random action\n            return random.randint(0, self.action_space_size - 1)\n        else:\n            # Exploit: best known action\n            return int(np.argmax(self.q_table[state]))\n\n    def update_q_value(self, state: Tuple[int, int], action: int,\n                       reward: float, next_state: Tuple[int, int]):\n        """Update Q-value using Bellman equation"""\n        current_q = self.q_table[state][action]\n\n        # Calculate target Q-value\n        max_next_q = np.max(self.q_table[next_state])\n        target_q = reward + self.discount_factor * max_next_q\n\n        # Update Q-value\n        self.q_table[state][action] += self.learning_rate * (target_q - current_q)\n\nclass GridWorldEnvironment:\n    def __init__(self, width: int, height: int):\n        self.width = width\n        self.height = height\n        self.agent_pos = [0, 0]  # Start at top-left\n        self.goal_pos = [width-1, height-1]  # Goal at bottom-right\n        self.obstacles = [[2, 1], [2, 2], [3, 2]]  # Fixed obstacles\n\n        # Action space: 0=up, 1=right, 2=down, 3=left\n        self.actions = [(-1, 0), (0, 1), (1, 0), (0, -1)]\n\n    def reset(self):\n        """Reset environment to initial state"""\n        self.agent_pos = [0, 0]\n        return tuple(self.agent_pos)\n\n    def step(self, action: int) -> Tuple[Tuple[int, int], float, bool]:\n        """Execute action and return (next_state, reward, done)"""\n        # Calculate new position\n        new_pos = [\n            self.agent_pos[0] + self.actions[action][0],\n            self.agent_pos[1] + self.actions[action][1]\n        ]\n\n        # Check boundaries\n        if (0 <= new_pos[0] < self.width and 0 <= new_pos[1] < self.height):\n            # Check for obstacles\n            if new_pos not in self.obstacles:\n                self.agent_pos = new_pos\n\n        # Calculate reward\n        if self.agent_pos == self.goal_pos:\n            reward = 100  # Reached goal\n            done = True\n        elif self.agent_pos in self.obstacles:\n            reward = -10  # Hit obstacle\n            done = False\n        else:\n            reward = -1  # Time penalty\n            done = False\n\n        return tuple(self.agent_pos), reward, done\n\n# Example usage\nenv = GridWorldEnvironment(5, 5)\nagent = QLearningAgent(state_space_size=(5, 5), action_space_size=4)\n\n# Training loop\nepisodes = 1000\nepisode_rewards = []\n\nfor episode in range(episodes):\n    state = env.reset()\n    total_reward = 0\n    done = False\n\n    while not done:\n        action = agent.get_action(state)\n        next_state, reward, done = env.step(action)\n        agent.update_q_value(state, action, reward, next_state)\n        state = next_state\n        total_reward += reward\n\n    episode_rewards.append(total_reward)\n\nprint(f"Training completed. Final average reward: {np.mean(episode_rewards[-100:]):.2f}")\n'})}),"\n",(0,o.jsx)(e.h3,{id:"example-2-imitation-learning-for-robot-behavior",children:"Example 2: Imitation Learning for Robot Behavior"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'import numpy as np\nfrom typing import List, Tuple\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler\n\nclass ImitationLearningAgent:\n    def __init__(self):\n        self.demonstrations = []  # List of (state, action) pairs\n        self.model = LinearRegression()\n        self.scaler = StandardScaler()\n        self.trained = False\n\n    def add_demonstration(self, states: List[np.ndarray], actions: List[np.ndarray]):\n        """Add a demonstration trajectory"""\n        for state, action in zip(states, actions):\n            self.demonstrations.append((state, action))\n\n    def train(self):\n        """Train the imitation learning model"""\n        if len(self.demonstrations) == 0:\n            print("No demonstrations available for training")\n            return\n\n        # Prepare training data\n        X = np.array([demo[0] for demo in self.demonstrations])\n        y = np.array([demo[1] for demo in self.demonstrations])\n\n        # Normalize features\n        X_normalized = self.scaler.fit_transform(X)\n\n        # Train the model\n        self.model.fit(X_normalized, y)\n        self.trained = True\n        print(f"Trained on {len(self.demonstrations)} demonstration samples")\n\n    def predict_action(self, state: np.ndarray) -> np.ndarray:\n        """Predict action for given state"""\n        if not self.trained:\n            print("Model not trained yet")\n            return np.zeros_like(state)  # Return zero action if not trained\n\n        # Normalize state\n        state_normalized = self.scaler.transform([state])\n\n        # Predict action\n        predicted_action = self.model.predict(state_normalized)[0]\n        return predicted_action\n\nclass SimpleManipulationTask:\n    """Simulated manipulation task for demonstration"""\n    def __init__(self):\n        self.object_pos = np.array([0.5, 0.3, 0.1])  # Object position\n        self.robot_pos = np.array([0.0, 0.0, 0.5])   # Robot end-effector position\n        self.goal_pos = np.array([0.8, 0.7, 0.2])    # Goal position\n\n    def get_state(self) -> np.ndarray:\n        """Get current state (relative positions)"""\n        obj_to_robot = self.object_pos - self.robot_pos\n        robot_to_goal = self.goal_pos - self.robot_pos\n        return np.concatenate([obj_to_robot, robot_to_goal])\n\n    def execute_action(self, action: np.ndarray):\n        """Execute action and update environment"""\n        # Simple movement: update robot position\n        self.robot_pos += action * 0.05  # Scale down action\n        # Keep within bounds\n        self.robot_pos = np.clip(self.robot_pos, 0, 1)\n\n    def is_task_complete(self) -> bool:\n        """Check if task is complete"""\n        return np.linalg.norm(self.robot_pos - self.goal_pos) < 0.1\n\ndef generate_demonstration():\n    """Generate a demonstration trajectory"""\n    task = SimpleManipulationTask()\n    states = []\n    actions = []\n\n    # Simple demonstration: move toward object, then to goal\n    for _ in range(20):  # Move toward object\n        state = task.get_state()\n        # Simple policy: move toward object\n        action = task.object_pos - task.robot_pos\n        action = action / (np.linalg.norm(action) + 1e-6)  # Normalize\n        states.append(state)\n        actions.append(action)\n        task.execute_action(action)\n\n    # Reset task for second part\n    task.robot_pos = task.object_pos  # Start from object position\n\n    for _ in range(20):  # Move to goal\n        state = task.get_state()\n        # Simple policy: move toward goal\n        action = task.goal_pos - task.robot_pos\n        action = action / (np.linalg.norm(action) + 1e-6)  # Normalize\n        states.append(state)\n        actions.append(action)\n        task.execute_action(action)\n\n    return states, actions\n\n# Example usage\nimitation_agent = ImitationLearningAgent()\n\n# Generate and add demonstrations\nprint("Generating demonstrations...")\nfor i in range(5):  # Create 5 demonstration trajectories\n    states, actions = generate_demonstration()\n    imitation_agent.add_demonstration(states, actions)\n\n# Train the agent\nprint("Training imitation learning agent...")\nimitation_agent.train()\n\n# Test the trained agent\nprint("Testing trained agent...")\ntest_task = SimpleManipulationTask()\nfor step in range(30):\n    state = test_task.get_state()\n    action = imitation_agent.predict_action(state)\n    test_task.execute_action(action)\n\n    if test_task.is_task_complete():\n        print(f"Task completed at step {step}")\n        break\n\nprint(f"Final distance to goal: {np.linalg.norm(test_task.robot_pos - test_task.goal_pos):.3f}")\n'})}),"\n",(0,o.jsx)(e.h2,{id:"implementation-guide",children:"Implementation Guide"}),"\n",(0,o.jsx)(e.h3,{id:"step-1-setup",children:"Step 1: Setup"}),"\n",(0,o.jsx)(e.p,{children:"Install necessary libraries for learning-based control:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-bash",children:"pip install numpy scikit-learn matplotlib\n"})}),"\n",(0,o.jsx)(e.h3,{id:"step-2-implementation",children:"Step 2: Implementation"}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsx)(e.li,{children:"Define state and action spaces for the learning problem"}),"\n",(0,o.jsx)(e.li,{children:"Implement the learning algorithm (RL, imitation, or adaptive)"}),"\n",(0,o.jsx)(e.li,{children:"Create reward functions or demonstration collection methods"}),"\n",(0,o.jsx)(e.li,{children:"Add policy evaluation and improvement mechanisms"}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"step-3-testing",children:"Step 3: Testing"}),"\n",(0,o.jsx)(e.p,{children:"Validate your implementation by:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Testing learning convergence with simple problems"}),"\n",(0,o.jsx)(e.li,{children:"Verifying policy performance in simulation"}),"\n",(0,o.jsx)(e.li,{children:"Measuring learning efficiency and sample complexity"}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"hands-on-exercise",children:"Hands-on Exercise"}),"\n",(0,o.jsx)(e.p,{children:"Implement a learning-based control system:"}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Task 1"}),": Create a Q-learning agent for a simple navigation task"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Task 2"}),": Implement an imitation learning system for basic behaviors"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Task 3"}),": Add adaptive control mechanisms for parameter tuning"]}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"exercise-requirements",children:"Exercise Requirements"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Support both exploration and exploitation in learning"}),"\n",(0,o.jsx)(e.li,{children:"Include proper reward shaping for effective learning"}),"\n",(0,o.jsx)(e.li,{children:"Implement performance evaluation metrics"}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"verification",children:"Verification"}),"\n",(0,o.jsx)(e.p,{children:"How to test and validate the implementation:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Test learning convergence with known problems"}),"\n",(0,o.jsx)(e.li,{children:"Verify that learned policies outperform random policies"}),"\n",(0,o.jsx)(e.li,{children:"Measure sample efficiency and learning speed"}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"troubleshooting-guide",children:"Troubleshooting Guide"}),"\n",(0,o.jsx)(e.p,{children:"Common issues and solutions:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Issue 1"}),": Learning not converging or taking too long","\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Solution: Adjust learning rate, exploration rate, or reward structure"}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Issue 2"}),": Poor generalization to new situations","\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Solution: Collect more diverse demonstrations or use function approximation"}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Issue 3"}),": Instability in adaptive control","\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Solution: Add stability constraints and proper parameter bounds"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"real-world-relevance",children:"Real-World Relevance"}),"\n",(0,o.jsx)(e.p,{children:"Learning-based control is essential in humanoid robotics for:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Adapting to new environments and tasks"}),"\n",(0,o.jsx)(e.li,{children:"Improving performance through experience"}),"\n",(0,o.jsx)(e.li,{children:"Handling uncertain and dynamic conditions"}),"\n",(0,o.jsx)(e.li,{children:"Personalizing robot behavior to users"}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"safety-considerations",children:"Safety Considerations"}),"\n",(0,o.jsx)(e.p,{children:"When working with learning-based control systems:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Implement safety constraints to prevent dangerous behaviors"}),"\n",(0,o.jsx)(e.li,{children:"Use safe exploration techniques during learning"}),"\n",(0,o.jsx)(e.li,{children:"Validate learned policies before deployment"}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"further-exploration",children:"Further Exploration"}),"\n",(0,o.jsx)(e.p,{children:"Advanced topics and additional resources for students who want to dive deeper:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Study of deep reinforcement learning for complex control"}),"\n",(0,o.jsx)(e.li,{children:"Research into inverse reinforcement learning"}),"\n",(0,o.jsx)(e.li,{children:"Exploration of meta-learning for rapid adaptation"}),"\n",(0,o.jsx)(e.li,{children:"Investigation of human-in-the-loop learning approaches"}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,o.jsx)(e.p,{children:"This lesson covered the fundamentals of learning-based control for humanoid robots, including reinforcement learning, imitation learning, and adaptive control methods that enable robots to learn and improve their behaviors over time."}),"\n",(0,o.jsx)(e.h2,{id:"knowledge-check",children:"Knowledge Check"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Question 1: What are the main differences between reinforcement learning and imitation learning?"}),"\n",(0,o.jsx)(e.li,{children:"Question 2: How does adaptive control differ from traditional control methods?"}),"\n",(0,o.jsx)(e.li,{children:"Question 3: What are the challenges of applying learning-based methods to physical robots?"}),"\n"]})]})}function p(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,o.jsx)(e,{...n,children:(0,o.jsx)(c,{...n})}):c(n)}},8453:(n,e,t)=>{t.d(e,{R:()=>r,x:()=>s});var i=t(6540);const o={},a=i.createContext(o);function r(n){const e=i.useContext(a);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function s(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(o):n.components||o:r(n.components),i.createElement(a.Provider,{value:e},n.children)}}}]);